parametersdramatically.
Furthermore,othermethodsaresuggestedtohelpthenetworktolearn,aswell[41].
Poolinglayersreducethesizeoftheinputpatterntothenextconvolutionallayer. Batch
normalization,dropout,earlystopping,unsupervisedorsemiunsupervisedlearning,and
regularizationtechniquespreventthelearnednetworkfromoverfittingandincreasethe
learningabilityandspeed. TheAEandDBNareemployedasunsupervisedlearningand
then fine-tuned to avoid overfitting for limited labeled data. Long short-term memory
(LSTM)andgatedrecurrentunits(GRU)areRNNscapableofrevealingthelong-termtime
dependenciesofdatasamples.
2.3.1. ConvolutionalNeuralNetworks(CNNs)