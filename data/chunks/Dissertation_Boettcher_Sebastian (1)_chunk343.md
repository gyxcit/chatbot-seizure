treestoproduceinthe
model
Falsepositivecost 1,10,20,30,40,50* Specificmisclassification
costforfalsepositiveswhen
weightingduringthe
learningprocess
Treedepth 1*,2,4,8,-1 Themaximumnumberof
splitsinthedecisiontree,
where-1denotesoneless
thanthenumberofsamples
inthetrainingset,thatis,
themaximumpossiblevalue
*Thechosenoptimalparametercombination.
parameter combination highlighted. In total, 720 parameter combinations were evaluated in
thehyperparameteroptimizationprocess.
Furthermore, the GBT model also had some fixed parameters that were the same for all
optimization runs. The boosting method used in the model was adaptive boosting for binary